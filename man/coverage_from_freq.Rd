% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/coverage.R
\name{coverage_from_freq}
\alias{coverage_from_freq}
\title{Build Cumulative Coverage Analysis}
\usage{
coverage_from_freq(
  freq_tbl,
  p_col = "p",
  thresholds = c(0.5, 0.9),
  name_col = "word"
)
}
\arguments{
\item{freq_tbl}{A frequency table with at least a column for relative
frequency. For unigrams, should have "word" column. For n-grams,
should have w1..wn columns or a "term" column.}

\item{p_col}{Character. Name of the column containing relative frequency.
Default is "p".}

\item{thresholds}{Numeric vector. Cumulative coverage thresholds to analyze
(e.g., c(0.5, 0.9) for 50% and 90% coverage). Default is c(0.5, 0.9).}

\item{name_col}{Character. Name of the column containing term names.
Default is "word". If missing and w1..wn columns exist, a "term" column
will be created automatically.}
}
\value{
A list with two elements:
  \describe{
    \item{cum_tbl}{Tibble with rank, cumulative probability, and original data}
    \item{summary}{Tibble showing how many unique terms needed for each threshold}
  }
}
\description{
This function analyzes how many unique terms are needed to cover certain
percentages of the total corpus. This is useful for vocabulary reduction
and understanding text distributions (Zipf's law).
}
\details{
The analysis helps answer questions like:
\itemize{
  \item How many words cover 50% of all text?
  \item What vocabulary size is needed for 90% coverage?
  \item How does coverage relate to rank (Zipf distribution)?
}
}
\examples{
\dontrun{
# Analyze unigram coverage
corpus <- load_corpus("en_US")
unigrams <- tokenize_unigrams(corpus)
freq_table <- freq_unigrams(unigrams)

coverage <- coverage_from_freq(freq_table, thresholds = c(0.5, 0.8, 0.9))
print(coverage$summary)

# Analyze bigram coverage
bigrams <- tokenize_bigrams(corpus)
freq_bi <- freq_bigrams(bigrams)
coverage_bi <- coverage_from_freq(freq_bi, thresholds = c(0.5, 0.9))
}

}
