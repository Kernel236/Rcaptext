% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/perplexity.R
\name{merge_acc_ppl}
\alias{merge_acc_ppl}
\title{Merge Accuracy and Perplexity Tables for Model Comparison}
\usage{
merge_acc_ppl(df_acc, df_ppl, k_pick = 1)
}
\arguments{
\item{df_acc}{Tibble. Accuracy results with columns: model_id, k, accuracy.}

\item{df_ppl}{Tibble. Perplexity results with columns: model_id, scope, perplexity.}

\item{k_pick}{Integer. Which k value to extract from accuracy table (default: 1
for top-1 accuracy).}
}
\value{
Tibble with columns: model_id, accuracy (at k_pick), perplexity (overall).
}
\description{
Helper function to combine accuracy@k and perplexity metrics for comparing
multiple language models.
}
\details{
Use this to create a joint comparison table when evaluating multiple models
with different hyperparameters.
}
\examples{
\dontrun{
# Compare models
combined <- merge_acc_ppl(
  df_acc = accuracy_results,
  df_ppl = perplexity_results,
  k_pick = 1
)
print(combined)
}

}
