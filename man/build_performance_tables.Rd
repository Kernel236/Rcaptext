% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance.R
\name{build_performance_tables}
\alias{build_performance_tables}
\title{Build Comprehensive Performance Summary Tables}
\usage{
build_performance_tables(eval_res)
}
\arguments{
\item{eval_res}{List returned by \code{\link{evaluate_accuracy_at_k}}.
Must contain \code{accuracy} and \code{per_case} components.}
}
\value{
A list with the following components:
  \describe{
    \item{accuracy}{Tibble. Formatted accuracy@k with percentage strings}
    \item{timing}{Tibble. Latency statistics (mean, p50, p95 in ms), or NULL if not measured}
    \item{hit_breakdown}{Tibble. Counts of top-1 hits, top-K hits, and total test cases}
    \item{accuracy_by_source}{Tibble. Accuracy@k broken down by source category
      (blogs/news/twitter), only if \code{source} column exists in test data}
  }
}
\description{
Extracts and formats key performance metrics from evaluation results, including
accuracy@k, hit/miss counts, timing statistics, and optional per-source breakdown.
}
\details{
This function aggregates raw evaluation data into presentation-ready tables:
- **Hit breakdown**: Shows how many predictions were correct at top-1 vs anywhere in top-K
- **Timing**: Rounds latency metrics to 1 decimal place for readability
- **Source analysis**: Reveals if model performs differently on blogs vs news vs Twitter

Useful for generating reports, dashboards, or quick performance summaries.
}
\examples{
\dontrun{
results <- evaluate_accuracy_at_k(test_data, tri, bi, uni, timeit = TRUE)
perf <- build_performance_tables(results)

print(perf$accuracy)
print(perf$hit_breakdown)
print(perf$timing)
if (!is.null(perf$accuracy_by_source)) print(perf$accuracy_by_source)
}

}
\seealso{
\code{\link{evaluate_accuracy_at_k}} for generating evaluation results,
  \code{\link{summarise_and_plot_eval}} for one-line reporting
}
