% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/text_cleaning.R
\name{filter_non_english_unigrams}
\alias{filter_non_english_unigrams}
\title{Filter Non-English Unigrams from Frequency Table}
\usage{
filter_non_english_unigrams(
  freq_uni,
  dict = "en_US",
  min_len = 2,
  keep_stopwords = TRUE
)
}
\arguments{
\item{freq_uni}{A tibble with columns \code{word}, \code{n}, and \code{p}.
Typically output from \code{\link{freq_unigrams}}.}

\item{dict}{Character or hunspell dictionary object. Dictionary for spell checking.
Default is "en_US".}

\item{min_len}{Integer. Minimum word length to spell-check. Words shorter than
this are automatically removed. Default is 2.}

\item{keep_stopwords}{Logical. If TRUE, preserves standard English stopwords
even if flagged by spell checker. Default is TRUE.}
}
\value{
A filtered tibble with the same structure as input, but with
  non-English words removed and frequencies re-normalized.
}
\description{
This function removes likely non-English words from a unigram frequency table,
with options to preserve common stopwords and handle short tokens.
}
\details{
The function performs these steps:
\itemize{
  \item Normalizes typographic apostrophes to straight quotes
  \item Removes very short words (length < min_len)
  \item Optionally preserves English stopwords as safety net
  \item Uses spell checking to identify non-English words
  \item Re-normalizes probabilities after filtering
}

The filtered count and types are stored as attributes "removed_n" and "removed_types".
}
\examples{
\dontrun{
# Assuming you have a frequency table
freq_table <- freq_unigrams(tokenize_unigrams(corpus))

# Filter non-English words
english_freq <- filter_non_english_unigrams(freq_table)

# Check what was removed
attr(english_freq, "removed_n")      # removed tokens
attr(english_freq, "removed_types")  # removed unique words

# More aggressive filtering
strict_freq <- filter_non_english_unigrams(
  freq_table, 
  min_len = 3, 
  keep_stopwords = FALSE
)
}

}
