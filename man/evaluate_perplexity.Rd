% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/perplexity.R
\name{evaluate_perplexity}
\alias{evaluate_perplexity}
\title{Evaluate Perplexity on Test Set using Stupid Backoff}
\usage{
evaluate_perplexity(
  test_windows,
  tri_pruned,
  bi_pruned,
  uni_lookup,
  alpha = 0.4,
  eps = 1e-09,
  by_source = TRUE,
  use_progress = TRUE
)
}
\arguments{
\item{test_windows}{Tibble. Test data with columns: w1, w2, target, and optionally
input_text and source. Typically from \code{\link{make_test_trigrams}}.}

\item{tri_pruned}{Data frame. Pruned trigram model with columns (w1, w2, w3, p_cond).}

\item{bi_pruned}{Data frame. Pruned bigram model with columns (w1, w2, p_cond).}

\item{uni_lookup}{Data frame. Unigram lookup table with columns (word, p).}

\item{alpha}{Numeric. Backoff penalty factor (default: 0.4).}

\item{eps}{Numeric. Smoothing value to avoid log(0) (default: 1e-9).}

\item{by_source}{Logical. If TRUE and 'source' column exists, computes per-source
perplexity in addition to overall (default: TRUE).}

\item{use_progress}{Logical. If TRUE, displays progress bar (default: TRUE).}
}
\value{
List with two components:
  \describe{
    \item{summary}{Tibble with columns: scope, N (token count), mean_logp,
      and perplexity. Contains overall metrics and per-source if requested.}
    \item{per_case}{Tibble with per-instance results: input_text, w1, w2,
      target, logp (log-probability), p (probability), and source (if present).}
  }
}
\description{
Computes perplexity metrics for a language model on a test set of trigram windows.
Perplexity measures how well the model predicts the test data (lower is better).
}
\details{
Perplexity is defined as exp(-mean_log_prob). Lower perplexity indicates better
model fit. Typical ranges:
\itemize{
  \item Excellent: < 50
  \item Good: 50-100
  \item Acceptable: 100-200
  \item Poor: > 200
}
}
\examples{
\dontrun{
# Evaluate perplexity on test set
ppl <- evaluate_perplexity(
  test_windows = test_data,
  tri_pruned = model$tri_pruned,
  bi_pruned = model$bi_pruned,
  uni_lookup = model$uni_lookup,
  alpha = 0.4
)
print(ppl$summary)
}

}
