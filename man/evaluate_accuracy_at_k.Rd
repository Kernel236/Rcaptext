% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/accuracy_evaluation.R
\name{evaluate_accuracy_at_k}
\alias{evaluate_accuracy_at_k}
\title{Evaluate Language Model Accuracy@k with Performance Metrics}
\usage{
evaluate_accuracy_at_k(
  test_windows,
  tri_pruned,
  bi_pruned,
  uni_lookup,
  alpha = 0.4,
  ks = c(1, 2, 3),
  timeit = TRUE,
  timing_n = 200,
  seed = 123,
  max_eval_cases = NULL,
  use_progress = TRUE
)
}
\arguments{
\item{test_windows}{Tibble. Test data with at least \code{input_text} (context string)
and \code{target} (ground truth next word). Typically output from
\code{\link{make_test_trigrams}}. May include \code{source} for stratified analysis.}

\item{tri_pruned}{Tibble. Pruned trigram model with columns (w1, w2, w3, p_cond).}

\item{bi_pruned}{Tibble. Pruned bigram model with columns (w1, w2, p_cond).}

\item{uni_lookup}{Tibble. Unigram lookup table with columns (word, n, p).}

\item{alpha}{Numeric. Backoff coefficient for Stupid Backoff algorithm (default: 0.4).
Controls penalty when backing off from trigram → bigram → unigram.}

\item{ks}{Integer vector. Values of k to evaluate, e.g., c(1, 3, 5) computes
accuracy@1, accuracy@3, and accuracy@5 (default: c(1, 2, 3)).}

\item{timeit}{Logical. If TRUE, measures prediction latency statistics (default: TRUE).}

\item{timing_n}{Integer. Number of random test cases to time for performance
profiling (default: 200). Useful for assessing real-time usability.}

\item{seed}{Integer. Random seed for reproducibility in timing sample (default: 123).}

\item{max_eval_cases}{Integer or NULL. Maximum number of test cases to evaluate
(default: NULL = use all). For large test sets (e.g., 2M+ cases), setting this
to 10K-50K dramatically speeds up evaluation with minimal loss in accuracy
estimates. Samples randomly if provided.}

\item{use_progress}{Logical. If TRUE, displays progress bars using progressr
and informative messages using cli (default: TRUE).}
}
\value{
A list with three components:
  \describe{
    \item{accuracy}{Tibble with columns \code{k} and \code{accuracy}. Shows
      proportion of test cases where target appears in top-k predictions.}
    \item{per_case}{Tibble with detailed per-instance results:
      \itemize{
        \item \code{input_text}: Input context string
        \item \code{target}: Ground truth next word
        \item \code{pred1, pred2, ..., predK}: Top-K predictions from model
        \item \code{rank_hit}: Position where target first appears (1-based), or NA if not in top-K
      }
      Useful for error analysis and understanding model failures.}
    \item{timing}{Tibble with latency statistics (only if \code{timeit=TRUE}):
      \itemize{
        \item \code{mean_ms}: Average prediction time in milliseconds
        \item \code{p50_ms}: Median latency (50th percentile)
        \item \code{p95_ms}: 95th percentile latency (tail latency)
        \item \code{n_calls}: Number of predictions timed
      }
      Critical for assessing real-time keyboard usability (target: <100ms).}
  }
}
\description{
Computes accuracy@k metrics for a next-word prediction model, measuring how often
the correct target word appears in the top-k predictions. This is the standard
evaluation metric for keyboard prediction and text suggestion systems.
}
\details{
**Accuracy@k Interpretation:**
- **Accuracy@1**: Strict metric - target must be THE top prediction (hardest)
- **Accuracy@3**: Practical for mobile keyboards showing 3 suggestions
- **Accuracy@5**: More lenient, suitable for desktop autocomplete

**How It Works:**
\enumerate{
  \item For each test case, generates top-K predictions using \code{predict_next()}
  \item Checks if \code{target} appears anywhere in top-K predictions
  \item Aggregates hits across all test cases to compute accuracy@k
  \item Optionally measures prediction latency for performance profiling
}

**Performance Benchmarks:**
- Mobile keyboards: <50ms per prediction (p95)
- Desktop autocomplete: <100ms acceptable
- Batch processing: latency less critical

**Note:** Requires a helper function \code{get_pred_vec()} that wraps
\code{\link{predict_next}} to return a character vector of predictions.
}
\examples{
\dontrun{
# Create train/test split
splits <- split_corpus(corpus, prop_test = 0.1)

# Build model
model <- build_model(splits$train, sample_prop = 0.1)

# Generate test cases
test_data <- make_test_trigrams(splits$test, prop = 0.2)

# Evaluate accuracy@k with timing
results <- evaluate_accuracy_at_k(
  test_windows = test_data,
  tri_pruned = model$tri_pruned,
  bi_pruned = model$bi_pruned,
  uni_lookup = model$uni_lookup,
  ks = c(1, 3, 5),
  timeit = TRUE
)

# View aggregate accuracy
print(results$accuracy)
#   k  accuracy
# 1 1  0.182
# 2 3  0.341
# 3 5  0.428

# Check latency
print(results$timing)
#   mean_ms  p50_ms  p95_ms  n_calls
# 1    12.3    10.5    28.4      200

# Analyze failures
failures <- results$per_case \%>\%
  filter(is.na(rank_hit)) \%>\%
  head(10)
}

}
\seealso{
\code{\link{make_test_trigrams}} for generating test data,
\code{\link{predict_next}} for the underlying prediction function,
\code{\link{build_model}} for training the model
}
