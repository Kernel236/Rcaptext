% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prediction.R
\name{predict_next}
\alias{predict_next}
\title{Predict next word using Stupid Backoff algorithm}
\usage{
predict_next(input, tri_pruned, bi_pruned, uni_lookup, alpha = 0.4, top_k = 3)
}
\arguments{
\item{input}{Character string. The input text to predict continuations for.
Will be cleaned and tokenized automatically using \code{extract_last_tokens}.}

\item{tri_pruned}{Data frame. Trigram model containing columns:
\itemize{
  \item \code{w1}: First word of trigram (character)
  \item \code{w2}: Second word of trigram (character) 
  \item \code{w3}: Third word of trigram (character)
  \item \code{p_cond}: Conditional probability P(w3 | w1, w2) (numeric)
}}

\item{bi_pruned}{Data frame. Bigram model containing columns:
\itemize{
  \item \code{w1}: First word of bigram (character)
  \item \code{w2}: Second word of bigram (character)
  \item \code{p_cond}: Conditional probability P(w2 | w1) (numeric) 
}}

\item{uni_lookup}{Data frame. Unigram model containing columns:
\itemize{
  \item \code{next}: Word (character)
  \item \code{p}: Marginal probability (numeric)
}}

\item{alpha}{Numeric. Backoff penalty factor (default: 0.4). Applied as:
\itemize{
  \item Trigram scores: original p_cond (no penalty)
  \item Bigram scores: alpha * p_cond  
  \item Unigram scores: alpha^2 * p
}
Lower values give stronger preference to higher-order n-grams.}

\item{top_k}{Integer. Number of predictions to return (default: 3).
Returns the top_k highest-scoring word predictions.}
}
\value{
Data frame with top_k predicted words containing columns:
  \itemize{
    \item \code{word}: Predicted next word (character)
    \item \code{score}: Prediction confidence score (numeric, 0-1 range)
    \item \code{source}: Model source - "trigram", "bigram", or "unigram" (character)
  }
  Ordered by decreasing score (most confident predictions first).
}
\description{
Predicts the most likely next words given input text using a Stupid Backoff
language model. The algorithm tries trigram predictions first, then backs off
to bigram and unigram models with decreasing confidence scores. This provides
robust predictions even when higher-order n-grams are not available.
}
\details{
The Stupid Backoff algorithm works as follows:
\enumerate{
  \item Extract last 1-2 words from input using \code{extract_last_tokens}
  \item Try trigram model: P(w3 | w1, w2) if 2 context words available
  \item Try bigram model: alpha * P(w2 | w1) using last context word
  \item Try unigram model: alpha^2 * P(w) as final fallback
  \item Combine all candidates, keeping best score per word
  \item Return top_k highest-scoring predictions
}
}
\examples{
\dontrun{
# Assuming you have trained models from build_pruned_lang_model()
lang_model <- build_pruned_lang_model(freq_uni, freq_bi, freq_tri)

# Predict next word after "I love"
predictions <- predict_next(
  input = "I love",
  tri_pruned = lang_model$tri_pruned,
  bi_pruned = lang_model$bi_pruned, 
  uni_lookup = lang_model$uni_lookup,
  alpha = 0.4,
  top_k = 5
)

print(predictions)
#   word score   source
# 1  you  0.35  trigram
# 2   it  0.28   bigram  
# 3 this  0.15 unigram

# Handle empty/short input
empty_pred <- predict_next("", lang_model$tri_pruned, 
                          lang_model$bi_pruned, lang_model$uni_lookup)
# Returns top unigrams with alpha^2 penalty

# Single word input (only bigram + unigram possible)
single_pred <- predict_next("hello", lang_model$tri_pruned,
                           lang_model$bi_pruned, lang_model$uni_lookup) 
}

}
