% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build_model.R
\name{build_model}
\alias{build_model}
\title{Build Complete N-gram Language Model from Training Corpus}
\usage{
build_model(
  train_corpus,
  text_col = "text_clean",
  ensure_clean = TRUE,
  min_chars = 1,
  sample_prop = NULL,
  seed = 123,
  oov_filter = TRUE,
  dict = "en_US",
  min_len_oov = 2,
  keep_stopwords = TRUE,
  min_count_bi = 2,
  min_count_tri = 2,
  topN_bi = 12,
  topN_tri = 8,
  out_dir = "data/processed",
  save = TRUE,
  return_freq = FALSE
)
}
\arguments{
\item{train_corpus}{Tibble. Training data with at least \code{source} column
and a text column (cleaned or raw).}

\item{text_col}{Character. Name of the cleaned text column (default: "text_clean").}

\item{ensure_clean}{Logical. If TRUE and \code{text_col} is missing, creates it
from \code{text} column using \code{\link{clean_text}} (default: TRUE).}

\item{min_chars}{Integer. Drop rows with fewer than this many characters in
\code{text_col} before tokenization (default: 1).}

\item{sample_prop}{Numeric or NULL. If numeric (e.g., 0.1), samples this proportion
of rows within each source for RAM control. If NULL, uses all data (default: NULL).}

\item{seed}{Integer. Random seed for reproducibility, used in sampling (default: 123).}

\item{oov_filter}{Logical. If TRUE, applies \code{\link{filter_non_english_unigrams}}
to remove likely foreign/misspelled words (default: TRUE).}

\item{dict}{Character. Hunspell dictionary for OOV filtering (default: "en_US").
Only used if \code{oov_filter=TRUE}.}

\item{min_len_oov}{Integer. Minimum word length when filtering non-English words
(default: 2). Shorter words are automatically removed.}

\item{keep_stopwords}{Logical. Keep English stopwords when filtering non-English
(default: TRUE). Recommended TRUE for next-word prediction models.}

\item{min_count_bi}{Integer. Pruning threshold for bigrams - removes bigrams with
count < min_count_bi (default: 2).}

\item{min_count_tri}{Integer. Pruning threshold for trigrams - removes trigrams with
count < min_count_tri (default: 2).}

\item{topN_bi}{Integer. Keep only top-N most probable bigram continuations per
history word w1 (default: 12).}

\item{topN_tri}{Integer. Keep only top-N most probable trigram continuations per
history pair (w1, w2) (default: 8).}

\item{out_dir}{Character. Directory where RDS files will be written (default: "data/processed").
Created automatically if missing.}

\item{save}{Logical. If TRUE, writes RDS files: \code{uni_lookup.rds},
\code{bi_pruned.rds}, \code{tri_pruned.rds}, \code{lang_meta.rds} (default: TRUE).}

\item{return_freq}{Logical. If TRUE, includes raw frequency tables in the returned
list (can be large). Useful for debugging (default: FALSE).}
}
\value{
A list containing the complete language model:
  \describe{
    \item{uni_lookup}{Tibble. Unigram lookup table (word, n, p)}
    \item{bi_pruned}{Tibble. Pruned bigram model (w1, w2, n, p_cond)}
    \item{tri_pruned}{Tibble. Pruned trigram model (w1, w2, w3, n, p_cond)}
    \item{meta}{List. Metadata (timestamp, parameters, sizes before/after pruning)}
    \item{freq_uni, freq_bi, freq_tri}{(Optional) Raw frequency tables if return_freq=TRUE}
  }
}
\description{
End-to-end pipeline that takes a training corpus and produces a complete,
pruned n-gram language model ready for text prediction. Handles text cleaning,
tokenization, frequency computation, optional foreign word filtering, and
model pruning with automatic persistence to disk.
}
\details{
The pipeline executes these steps:
\enumerate{
  \item **Text Preparation**: Ensures cleaned text exists, filters short/empty lines
  \item **Optional Sampling**: Reduces corpus size for RAM control (stratified by source)
  \item **Tokenization**: Creates unigrams, bigrams, and trigrams using tidytext
  \item **Frequency Computation**: Counts n-gram occurrences and computes probabilities
  \item **OOV Filtering** (optional): Removes likely non-English words using hunspell
  \item **Model Building**: Computes conditional probabilities P(w2|w1) and P(w3|w1,w2)
  \item **Pruning**: Removes rare n-grams and keeps only top-N per history
  \item **Persistence**: Saves pruned tables to RDS for fast loading
}

Memory optimization tips:
- Use \code{sample_prop} to work with a fraction of data
- Increase \code{min_count_bi} and \code{min_count_tri} to prune more aggressively
- Decrease \code{topN_bi} and \code{topN_tri} for smaller models
- Set \code{return_freq=FALSE} to avoid keeping large frequency tables in memory
}
\examples{
\dontrun{
# Basic usage with full corpus
corpus <- load_corpus("en_US")
splits <- split_corpus(corpus, prop_test = 0.1)

model <- build_model(
  train_corpus = splits$train,
  text_col = "text_clean",
  out_dir = "models/my_model"
)

# Memory-efficient: sample 10\% + aggressive pruning
model_small <- build_model(
  train_corpus = splits$train,
  sample_prop = 0.1,
  min_count_bi = 5,
  min_count_tri = 3,
  topN_bi = 8,
  topN_tri = 5
)

# Without OOV filtering (faster but includes foreign words)
model_fast <- build_model(
  train_corpus = splits$train,
  oov_filter = FALSE
)

# Check model statistics
print(model$meta$sizes)
}

}
\seealso{
\code{\link{split_corpus}} for creating train/test splits,
\code{\link{build_pruned_lang_model}} for the underlying model builder,
\code{\link{predict_next}} for using the model
}
